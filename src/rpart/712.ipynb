{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necesita para correr en Google Cloud\n",
    "#32 GB de memoria RAM\n",
    "#256 GB de espacio en el disco local\n",
    "#8 vCPU\n",
    "\n",
    "#clase_binaria2   1={BAJA+2,BAJA+1}    0={CONTINUA}\n",
    "#Entrena en UN SOLO mes [202011]\n",
    "#Usa variables historicas de lag de orden uno y  delta lag\n",
    "\n",
    "#Optimizacion Bayesiana de hiperparametros de  lightgbm\n",
    "#usa el interminable  5-fold cross validation\n",
    "#funciona automaticamente con EXPERIMENTOS\n",
    "#va generando incrementalmente salidas para kaggle\n",
    "\n",
    "# WARNING  usted debe cambiar este script si lo corre en su propio Linux\n",
    "\n",
    "#limpio la memoria\n",
    "rm( list=ls() )  #remove all objects\n",
    "gc()             #garbage collection\n",
    "\n",
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"yaml\")\n",
    "\n",
    "require(\"lightgbm\")\n",
    "\n",
    "#paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "\n",
    "#para poder usarlo en la PC y en la nube sin tener que cambiar la ruta\n",
    "#cambiar aqui las rutas en su maquina\n",
    "switch ( Sys.info()[['sysname']],\n",
    "         Windows = { directory.root  <-  \"M:\\\\\" },   #Windows\n",
    "         Darwin  = { directory.root  <-  \"~/dm/\" },  #Apple MAC\n",
    "         Linux   = { directory.root  <-  \"~/buckets/b1/\" } #Google Cloud\n",
    "       )\n",
    "#defino la carpeta donde trabajo\n",
    "setwd( directory.root )\n",
    "\n",
    "\n",
    "\n",
    "kexperimento  <- NA   #NA si se corre la primera vez, un valor concreto si es para continuar procesando\n",
    "\n",
    "kscript         <- \"712_lgb_bin2_lagdelta\"\n",
    "\n",
    "karch_dataset    <- \"./datasetsOri/paquete_premium.csv.gz\"\n",
    "kmes_apply       <- 202101  #El mes donde debo aplicar el modelo\n",
    "kmes_train_hasta <- 202011  #Obvimente, solo puedo entrenar hasta 202011\n",
    "\n",
    "kmes_train_desde <- 202011  #Entreno desde 202011, o sea solo entreno en noviembre-2020\n",
    "\n",
    "kcanaritos  <-   90\n",
    "kBO_iter    <-  100   #cantidad de iteraciones de la Optimizacion Bayesiana\n",
    "\n",
    "#Aqui se cargan los hiperparametros\n",
    "hs <- makeParamSet( \n",
    "         makeNumericParam(\"learning_rate\",    lower=    0.02 , upper=    0.06),\n",
    "         makeNumericParam(\"feature_fraction\", lower=    0.1  , upper=    0.4),\n",
    "         makeIntegerParam(\"min_data_in_leaf\", lower= 1000L   , upper= 8000L),\n",
    "         makeIntegerParam(\"num_leaves\",       lower=  100L   , upper= 1024L),\n",
    "         makeNumericParam(\"prob_corte\",       lower=    0.040, upper=    0.055)\n",
    "        )\n",
    "\n",
    "campos_malos  <- c( )   #Intencionalmente VACIO\n",
    "\n",
    "ksemilla_azar  <- 102191  #Aqui poner la propia semilla\n",
    "#------------------------------------------------------------------------------\n",
    "#Funcion que lleva el registro de los experimentos\n",
    "\n",
    "get_experimento  <- function()\n",
    "{\n",
    "  if( !file.exists( \"./maestro.yaml\" ) )  cat( file=\"./maestro.yaml\", \"experimento: 1000\" )\n",
    "\n",
    "  exp  <- read_yaml( \"./maestro.yaml\" )\n",
    "  experimento_actual  <- exp$experimento\n",
    "\n",
    "  exp$experimento  <- as.integer(exp$experimento + 1)\n",
    "  Sys.chmod( \"./maestro.yaml\", mode = \"0644\", use_umask = TRUE)\n",
    "  write_yaml( exp, \"./maestro.yaml\" )\n",
    "  Sys.chmod( \"./maestro.yaml\", mode = \"0444\", use_umask = TRUE) #dejo el archivo readonly\n",
    "\n",
    "  return( experimento_actual )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "#graba a un archivo los componentes de lista\n",
    "#para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear  <- function( reg, arch=NA, folder=\"./work/\", ext=\".txt\", verbose=TRUE )\n",
    "{\n",
    "  archivo  <- arch\n",
    "  if( is.na(arch) )  archivo  <- paste0(  folder, substitute( reg), ext )\n",
    "\n",
    "  if( !file.exists( archivo ) )  #Escribo los titulos\n",
    "  {\n",
    "    linea  <- paste0( \"fecha\\t\", \n",
    "                      paste( list.names(reg), collapse=\"\\t\" ), \"\\n\" )\n",
    "\n",
    "    cat( linea, file=archivo )\n",
    "  }\n",
    "\n",
    "  linea  <- paste0( format(Sys.time(), \"%Y%m%d %H%M%S\"),  \"\\t\",     #la fecha y hora\n",
    "                    gsub( \", \", \"\\t\", toString( reg ) ),  \"\\n\" )\n",
    "\n",
    "  cat( linea, file=archivo, append=TRUE )  #grabo al archivo\n",
    "\n",
    "  if( verbose )  cat( linea )   #imprimo por pantalla\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "PROB_CORTE  <- 0.025\n",
    "\n",
    "fganancia_logistic_lightgbm   <- function(probs, datos) \n",
    "{\n",
    "  vlabels  <- getinfo(datos, \"label\")\n",
    "  vpesos   <- getinfo(datos, \"weight\")\n",
    "\n",
    "  #aqui esta el inmoral uso de los pesos para calcular la ganancia correcta\n",
    "  gan  <- sum( (probs > PROB_CORTE  ) *\n",
    "               ifelse( vlabels== 1 & vpesos > 1, 48750, -1250 ) )\n",
    "\n",
    "  return( list( \"name\"= \"ganancia\", \n",
    "                \"value\"=  gan,\n",
    "                \"higher_better\"= TRUE ) )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "#esta funcion solo puede recibir los parametros que se estan optimizando\n",
    "#el resto de los parametros se pasan como variables globales, la semilla del mal ...\n",
    "\n",
    "EstimarGanancia_lightgbm  <- function( x )\n",
    "{\n",
    "  GLOBAL_iteracion  <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  gc()\n",
    "  PROB_CORTE <<- x$prob_corte   #asigno la variable global\n",
    "\n",
    "  kfolds  <- 5   # cantidad de folds para cross validation\n",
    "\n",
    "  param_basicos  <- list( objective= \"binary\",\n",
    "                          metric= \"custom\",\n",
    "                          first_metric_only= TRUE,\n",
    "                          boost_from_average= TRUE,\n",
    "                          feature_pre_filter= FALSE,\n",
    "                          verbosity= -100,\n",
    "                          seed= 999983,\n",
    "                          max_depth=  -1,         # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "                          min_gain_to_split= 0.0, #por ahora, lo dejo fijo\n",
    "                          lambda_l1= 0.0,         #por ahora, lo dejo fijo\n",
    "                          lambda_l2= 0.0,         #por ahora, lo dejo fijo\n",
    "                          max_bin= 31,            #por ahora, lo dejo fijo\n",
    "                          num_iterations= 9999,    #un numero muy grande, lo limita early_stopping_rounds\n",
    "                          force_row_wise= TRUE    #para que los alumnos no se atemoricen con tantos warning\n",
    "                        )\n",
    "\n",
    "  #el parametro discolo, que depende de otro\n",
    "  param_variable  <- list(  early_stopping_rounds= as.integer(50 + 1/x$learning_rate) )\n",
    "\n",
    "  param_completo  <- c( param_basicos, param_variable, x )\n",
    "\n",
    "  set.seed( 999983 )\n",
    "  modelocv  <- lgb.cv( data= dtrain,\n",
    "                       eval= fganancia_logistic_lightgbm,\n",
    "                       stratified= TRUE, #sobre el cross validation\n",
    "                       nfold= kfolds,    #folds del cross validation\n",
    "                       param= param_completo,\n",
    "                       verbose= -100\n",
    "                      )\n",
    "\n",
    "\n",
    "  ganancia  <- unlist(modelocv$record_evals$valid$ganancia$eval)[ modelocv$best_iter ]\n",
    "\n",
    "  ganancia_normalizada  <-  ganancia* kfolds  \n",
    "  attr(ganancia_normalizada ,\"extras\" )  <- list(\"num_iterations\"= modelocv$best_iter)  #esta es la forma de devolver un parametro extra\n",
    "\n",
    "  param_completo$num_iterations <- modelocv$best_iter  #asigno el mejor num_iterations\n",
    "  param_completo[\"early_stopping_rounds\"]  <- NULL\n",
    "\n",
    "   #si tengo una ganancia superadora, genero el archivo para Kaggle\n",
    "   if(  ganancia > GLOBAL_ganancia_max )\n",
    "   {\n",
    "     GLOBAL_ganancia_max  <<- ganancia  #asigno la nueva maxima ganancia a una variable GLOBAL, por eso el <<-\n",
    "\n",
    "     set.seed(ksemilla_azar)\n",
    "\n",
    "     modelo  <- lightgbm( data= dtrain,\n",
    "                          param= param_completo,\n",
    "                          verbose= -100\n",
    "                        )\n",
    "\n",
    "    #calculo la importancia de variables\n",
    "    tb_importancia  <- lgb.importance( model= modelo )\n",
    "    fwrite( tb_importancia, \n",
    "            file= paste0(kimp, \"imp_\", GLOBAL_iteracion, \".txt\"),\n",
    "            sep=\"\\t\" )\n",
    "\n",
    "     prediccion  <- predict( modelo, data.matrix( dapply[  , campos_buenos, with=FALSE]) )\n",
    "\n",
    "     Predicted  <- as.integer( prediccion > x$prob_corte )\n",
    "\n",
    "     entrega  <- as.data.table( list( \"numero_de_cliente\"= dapply$numero_de_cliente, \n",
    "                                      \"Predicted\"= Predicted)  )\n",
    "\n",
    "     #genero el archivo para Kaggle\n",
    "     fwrite( entrega, \n",
    "             file= paste0(kkaggle, GLOBAL_iteracion, \".csv\" ),\n",
    "             sep= \",\" )\n",
    "   }\n",
    "\n",
    "   #logueo \n",
    "   xx  <- param_completo\n",
    "   xx$iteracion_bayesiana  <- GLOBAL_iteracion\n",
    "   xx$ganancia  <- ganancia_normalizada   #le agrego la ganancia\n",
    "   loguear( xx,  arch= klog )\n",
    "\n",
    "   return( ganancia )\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "#Aqui empieza el programa\n",
    "\n",
    "if( is.na(kexperimento ) )   kexperimento <- get_experimento()  #creo el experimento\n",
    "\n",
    "#en estos archivos quedan los resultados\n",
    "dir.create( paste0( \"./work/E\",  kexperimento, \"/\" ) )\n",
    "kbayesiana  <- paste0(\"./work/E\",  kexperimento, \"/E\",  kexperimento, \"_\", kscript, \".RDATA\" )\n",
    "klog        <- paste0(\"./work/E\",  kexperimento, \"/E\",  kexperimento, \"_\", kscript, \"_BOlog.txt\" )\n",
    "kimp        <- paste0(\"./work/E\",  kexperimento, \"/E\",  kexperimento, \"_\", kscript, \"_\" )\n",
    "kkaggle     <- paste0(\"./kaggle/E\",kexperimento, \"_\", kscript, \"_\" )\n",
    "\n",
    "\n",
    "GLOBAL_ganancia_max  <-  -Inf\n",
    "GLOBAL_iteracion  <- 0\n",
    "\n",
    "#si ya existe el archivo log, traigo hasta donde llegue\n",
    "if( file.exists(klog) )\n",
    "{\n",
    " tabla_log  <- fread( klog)\n",
    " GLOBAL_iteracion  <- nrow( tabla_log ) -1\n",
    " GLOBAL_ganancia_max  <- tabla_log[ , max(ganancia) ]\n",
    "}\n",
    "\n",
    "\n",
    "#cargo el dataset que tiene los 36 meses\n",
    "dataset  <- fread(karch_dataset)\n",
    "\n",
    "campos_lags  <- setdiff(  colnames(dataset) ,  c(\"clase_ternaria\",\"clase01\", \"numero_de_cliente\",\"foto_mes\", campos_malos) )\n",
    "\n",
    "#Hago feature Engineering en este mismo script\n",
    "#agreglo los lags de orden 1\n",
    "setorderv( dataset, c(\"numero_de_cliente\",\"foto_mes\") )\n",
    "dataset[ , paste0( campos_lags, \"_lag1\") := shift(.SD, 1, NA, \"lag\"), \n",
    "           by= numero_de_cliente, \n",
    "           .SDcols= campos_lags]\n",
    "\n",
    "#agrego los deltas de los lags, con un \"for\" nada elegante\n",
    "for( vcol in campos_lags )\n",
    "{\n",
    "   dataset[,  paste0(vcol, \"_delta1\") := get( vcol)  - get(paste0( vcol, \"_lag1\"))]\n",
    "}\n",
    "\n",
    "\n",
    "#Agrego los canaritos\n",
    "if( kcanaritos > 0 )\n",
    "{\n",
    "  for( i  in 1:kcanaritos)  dataset[ , paste0(\"canarito\", i ) :=  runif( nrow(dataset))]\n",
    "}\n",
    "\n",
    "\n",
    "#cargo los datos donde voy a aplicar el modelo\n",
    "dapply  <- copy( dataset[  foto_mes==kmes_apply ] )\n",
    "\n",
    "\n",
    "#creo la clase_binaria2   1={ BAJA+2,BAJA+1}  0={CONTINUA}\n",
    "dataset[ , clase01:= ifelse( clase_ternaria==\"CONTINUA\", 0, 1 ) ]\n",
    "\n",
    "\n",
    "\n",
    "#los campos que se van a utilizar\n",
    "campos_buenos  <- setdiff( colnames(dataset), c(\"clase_ternaria\",\"clase01\", campos_malos) )\n",
    "\n",
    "#dejo los datos en el formato que necesita LightGBM\n",
    "#uso el weight como un truco ESPANTOSO para saber la clase real\n",
    "dtrain  <- lgb.Dataset( data= data.matrix(  dataset[  foto_mes>=kmes_train_desde & foto_mes<=kmes_train_hasta , campos_buenos, with=FALSE]),\n",
    "                        label=  dataset[ foto_mes>=kmes_train_desde & foto_mes<=kmes_train_hasta, clase01],\n",
    "                        weight=  dataset[ foto_mes>=kmes_train_desde & foto_mes<=kmes_train_hasta , ifelse(clase_ternaria==\"BAJA+2\", 1.0000001, 1.0)] ,\n",
    "                        free_raw_data= TRUE\n",
    "                      )\n",
    "\n",
    "#elimino el dataset para liberar memoria RAM\n",
    "rm( dataset )\n",
    "gc()\n",
    "\n",
    "#Aqui comienza la configuracion de la Bayesian Optimization\n",
    "\n",
    "funcion_optimizar  <- EstimarGanancia_lightgbm   #la funcion que voy a maximizar\n",
    "\n",
    "configureMlr( show.learner.output= FALSE)\n",
    "\n",
    "#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "#por favor, no desesperarse por lo complejo\n",
    "obj.fun  <- makeSingleObjectiveFunction(\n",
    "              fn=       funcion_optimizar, #la funcion que voy a maximizar\n",
    "              minimize= FALSE,   #estoy Maximizando la ganancia\n",
    "              noisy=    TRUE,\n",
    "              par.set=  hs,     #definido al comienzo del programa\n",
    "              has.simple.signature = FALSE   #paso los parametros en una lista\n",
    "             )\n",
    "\n",
    "ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  save.file.path= kbayesiana)  #se graba cada 600 segundos\n",
    "ctrl  <- setMBOControlTermination(ctrl, iters= kBO_iter )   #cantidad de iteraciones\n",
    "ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI() )\n",
    "\n",
    "#establezco la funcion que busca el maximo\n",
    "surr.km  <- makeLearner(\"regr.km\", predict.type= \"se\", covtype= \"matern3_2\", control= list(trace= TRUE))\n",
    "\n",
    "#inicio la optimizacion bayesiana\n",
    "if(!file.exists(kbayesiana)) {\n",
    "  run  <- mbo(obj.fun, learner= surr.km, control= ctrl)\n",
    "} else {\n",
    "  run  <- mboContinue( kbayesiana )   #retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#apagado de la maquina virtual, pero NO se borra\n",
    "system( \"sleep 10  &&  sudo shutdown -h now\", wait=FALSE)\n",
    "\n",
    "#suicidio,  elimina la maquina virtual directamente\n",
    "#system( \"sleep 10  && \n",
    "#        export NAME=$(curl -X GET http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google') &&\n",
    "#        export ZONE=$(curl -X GET http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google') &&\n",
    "#        gcloud --quiet compute instances delete $NAME --zone=$ZONE\",\n",
    "#        wait=FALSE )\n",
    "\n",
    "\n",
    "quit( save=\"no\" )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
